{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ccfbfbe8-3658-4343-a683-61eb5f9366a9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os, sys\n",
    "from glob import glob\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import albumentations as A\n",
    "from torchvision.models import alexnet\n",
    "\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "import cv2 as cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ea2b9cba-13c5-45aa-9e52-3b43289a3f7c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:\\GitCloneProject\\CE-AB-CLS\\OtherSOTA\n"
     ]
    }
   ],
   "source": [
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "54ce39ee-63c5-4266-9b3f-7e935c48f7d0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "os.chdir(\"..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6241e45d-7ef4-4384-bdbc-db5833f49934",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Test', 'Train']\n"
     ]
    }
   ],
   "source": [
    "data_dir = os.getcwd() + \"\\\\Data set\\\\Original Form\"\n",
    "print(os.listdir(data_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c732268b-a989-4bc1-bede-c90fe6ef42d3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class CEDataset(Dataset):\n",
    "    def __init__(self, root_dir = data_dir, subset = \"Train\"):\n",
    "        self.root_dir = root_dir + f\"\\\\{subset}\"\n",
    "        self.img_paths = glob(self.root_dir + \"\\\\*\\\\*\")\n",
    "        self.transform = A.Compose([\n",
    "            # A.RandomCrop(height = 256, width = 256),\n",
    "            A.ColorJitter(p=0.5),\n",
    "            A.GaussianBlur(p=0.5),\n",
    "            A.RandomShadow(p=0.5),\n",
    "            A.RandomBrightnessContrast(p=0.2),\n",
    "            A.Resize(height = 256, width = 256, p = 1),\n",
    "        ])\n",
    "        super().__init__()\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.img_paths)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        path = self.img_paths[idx]\n",
    "        img = cv.imread(path)\n",
    "        \n",
    "        if \"Normal\" in path:\n",
    "            label = torch.tensor(0)\n",
    "        elif \"Abnormal\" in path:\n",
    "            label = torch.tensor(1)\n",
    "        else:\n",
    "            raise Exception(\"not shown type of label in path\")\n",
    "        \n",
    "        return torch.from_numpy(self.transform(image = img)[\"image\"]/255).permute(-1, 0, 1), label  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "304667a4-a584-42f1-91f6-df0d458e8d48",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "176\n",
      "176\n"
     ]
    }
   ],
   "source": [
    "train_dataset = CEDataset(subset = \"Train\")\n",
    "test_dataset = CEDataset(subset = \"Test\")\n",
    "\n",
    "print(len(train_dataset))\n",
    "print(len(test_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "208e49b4-1f28-4bd8-8967-4c7bd02af7d4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sample_img, sample_label = train_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "5fee4157-983b-4b1f-aef5-05ff86a58c81",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:\\GitCloneProject\\CE-AB-CLS\\Data set\\Original Form\\Train\\Group 1 - Normal\\CHGastro_Normal_001.png\n",
      "D:\\GitCloneProject\\CE-AB-CLS\\Data set\\Original Form\\Test\\Group 1 - Normal\\CHGastro_Normal_002.png\n"
     ]
    }
   ],
   "source": [
    "print(train_dataset.img_paths[0])\n",
    "print(test_dataset.img_paths[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "2fb3dd20-47d3-4b9d-b73f-a44f39640ef8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_dataset, shuffle = True, batch_size = 32)\n",
    "test_dataloader = DataLoader(test_dataset, shuffle = False, batch_size = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "345b8630-97cb-4309-9bdc-17ee1a39e991",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = alexnet(weights = 'IMAGENET1K_V1').to(device)\n",
    "model.fc = nn.Linear(1000, 2)\n",
    "# model.sf = nn.Softmax(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "17ca54ac-e22e-411c-8b46-5fc842a49379",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AlexNet(\n",
      "  (features): Sequential(\n",
      "    (0): Conv2d(3, 64, kernel_size=(11, 11), stride=(4, 4), padding=(2, 2))\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (3): Conv2d(64, 192, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "    (4): ReLU(inplace=True)\n",
      "    (5): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (6): Conv2d(192, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (7): ReLU(inplace=True)\n",
      "    (8): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (9): ReLU(inplace=True)\n",
      "    (10): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (11): ReLU(inplace=True)\n",
      "    (12): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=(6, 6))\n",
      "  (classifier): Sequential(\n",
      "    (0): Dropout(p=0.5, inplace=False)\n",
      "    (1): Linear(in_features=9216, out_features=4096, bias=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): Dropout(p=0.5, inplace=False)\n",
      "    (4): Linear(in_features=4096, out_features=4096, bias=True)\n",
      "    (5): ReLU(inplace=True)\n",
      "    (6): Linear(in_features=4096, out_features=1000, bias=True)\n",
      "  )\n",
      "  (fc): Linear(in_features=1000, out_features=2, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "5137660a-4089-443b-b12d-97ed8e32d7d5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "70bfcaec-bc9e-4fea-9e40-be340cfc53c2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 0.612485  [   32/    6] acc: 0.65625\n",
      "loss: 0.572555  [   32/    6] acc: 0.65625\n",
      "loss: 0.527337  [   32/    6] acc: 0.75\n",
      "loss: 0.567612  [   32/    6] acc: 0.71875\n",
      "loss: 0.606250  [   32/    6] acc: 0.65625\n",
      "loss: 0.671754  [   32/    6] acc: 0.53125\n",
      "loss: 0.680170  [   32/    6] acc: 0.625\n",
      "loss: 0.533879  [   32/    6] acc: 0.71875\n",
      "loss: 0.554774  [   32/    6] acc: 0.71875\n",
      "loss: 0.514703  [   32/    6] acc: 0.75\n",
      "loss: 0.612036  [   32/    6] acc: 0.625\n",
      "loss: 0.488404  [   32/    6] acc: 0.71875\n",
      "loss: 0.509546  [   32/    6] acc: 0.71875\n",
      "loss: 0.503870  [   32/    6] acc: 0.71875\n",
      "loss: 0.562823  [   32/    6] acc: 0.625\n",
      "loss: 0.473390  [   32/    6] acc: 0.84375\n",
      "loss: 0.453033  [   32/    6] acc: 0.78125\n",
      "loss: 0.487992  [   32/    6] acc: 0.78125\n",
      "loss: 0.877947  [   32/    6] acc: 0.34375\n",
      "loss: 0.591797  [   32/    6] acc: 0.65625\n",
      "loss: 0.568401  [   32/    6] acc: 0.71875\n",
      "loss: 0.496526  [   32/    6] acc: 0.8125\n",
      "loss: 0.534080  [   32/    6] acc: 0.6875\n",
      "loss: 0.388145  [   32/    6] acc: 0.8125\n",
      "loss: 0.601034  [   32/    6] acc: 0.75\n",
      "loss: 0.560411  [   32/    6] acc: 0.59375\n",
      "loss: 0.635927  [   32/    6] acc: 0.59375\n",
      "loss: 0.605070  [   32/    6] acc: 0.65625\n",
      "loss: 0.612073  [   32/    6] acc: 0.71875\n",
      "loss: 0.603572  [   32/    6] acc: 0.65625\n",
      "loss: 0.490488  [   32/    6] acc: 0.78125\n",
      "loss: 0.504200  [   32/    6] acc: 0.71875\n",
      "loss: 0.645979  [   32/    6] acc: 0.71875\n",
      "loss: 0.570535  [   32/    6] acc: 0.78125\n",
      "loss: 0.549243  [   32/    6] acc: 0.78125\n",
      "loss: 0.421345  [   32/    6] acc: 0.75\n",
      "loss: 0.577799  [   32/    6] acc: 0.6875\n",
      "loss: 0.354833  [   32/    6] acc: 0.8125\n",
      "loss: 0.560537  [   32/    6] acc: 0.65625\n",
      "loss: 0.800866  [   32/    6] acc: 0.6875\n",
      "loss: 0.503582  [   32/    6] acc: 0.75\n",
      "loss: 0.516582  [   32/    6] acc: 0.75\n",
      "loss: 0.501242  [   32/    6] acc: 0.6875\n",
      "loss: 0.487862  [   32/    6] acc: 0.8125\n",
      "loss: 1.087233  [   32/    6] acc: 0.59375\n",
      "loss: 0.743585  [   32/    6] acc: 0.71875\n",
      "loss: 0.519363  [   32/    6] acc: 0.71875\n",
      "loss: 0.500709  [   32/    6] acc: 0.78125\n",
      "loss: 0.582452  [   32/    6] acc: 0.71875\n",
      "loss: 0.464425  [   32/    6] acc: 0.78125\n",
      "loss: 0.531009  [   32/    6] acc: 0.6875\n",
      "loss: 0.577996  [   32/    6] acc: 0.75\n",
      "loss: 0.463118  [   32/    6] acc: 0.78125\n",
      "loss: 0.378476  [   32/    6] acc: 0.8125\n",
      "loss: 0.589340  [   32/    6] acc: 0.8125\n",
      "loss: 0.285018  [   32/    6] acc: 0.875\n",
      "loss: 0.520439  [   32/    6] acc: 0.65625\n",
      "loss: 0.385251  [   32/    6] acc: 0.84375\n",
      "loss: 0.518010  [   32/    6] acc: 0.78125\n",
      "loss: 0.250161  [   32/    6] acc: 0.90625\n",
      "loss: 0.617957  [   32/    6] acc: 0.59375\n",
      "loss: 0.574320  [   32/    6] acc: 0.65625\n",
      "loss: 0.377808  [   32/    6] acc: 0.84375\n",
      "loss: 0.568175  [   32/    6] acc: 0.71875\n",
      "loss: 0.575876  [   32/    6] acc: 0.75\n",
      "loss: 0.442771  [   32/    6] acc: 0.78125\n",
      "loss: 0.757523  [   32/    6] acc: 0.71875\n",
      "loss: 0.562753  [   32/    6] acc: 0.6875\n",
      "loss: 0.556152  [   32/    6] acc: 0.75\n",
      "loss: 0.364157  [   32/    6] acc: 0.9375\n",
      "loss: 0.369649  [   32/    6] acc: 0.84375\n",
      "loss: 0.395997  [   32/    6] acc: 0.84375\n",
      "loss: 0.322686  [   32/    6] acc: 0.875\n",
      "loss: 0.345514  [   32/    6] acc: 0.8125\n",
      "loss: 0.529385  [   32/    6] acc: 0.8125\n",
      "loss: 0.559068  [   32/    6] acc: 0.75\n",
      "loss: 0.368658  [   32/    6] acc: 0.78125\n",
      "loss: 0.511495  [   32/    6] acc: 0.78125\n",
      "loss: 0.465024  [   32/    6] acc: 0.75\n",
      "loss: 0.483800  [   32/    6] acc: 0.75\n",
      "loss: 0.321334  [   32/    6] acc: 0.90625\n",
      "loss: 0.462228  [   32/    6] acc: 0.8125\n",
      "loss: 0.485358  [   32/    6] acc: 0.78125\n",
      "loss: 0.493926  [   32/    6] acc: 0.6875\n",
      "loss: 0.554379  [   32/    6] acc: 0.71875\n",
      "loss: 0.503257  [   32/    6] acc: 0.75\n",
      "loss: 0.450544  [   32/    6] acc: 0.8125\n",
      "loss: 0.491738  [   32/    6] acc: 0.78125\n",
      "loss: 0.533570  [   32/    6] acc: 0.71875\n",
      "loss: 0.594525  [   32/    6] acc: 0.5625\n",
      "loss: 0.483987  [   32/    6] acc: 0.78125\n",
      "loss: 0.428527  [   32/    6] acc: 0.90625\n",
      "loss: 0.511789  [   32/    6] acc: 0.65625\n",
      "loss: 0.252183  [   32/    6] acc: 0.875\n",
      "loss: 0.609268  [   32/    6] acc: 0.625\n",
      "loss: 0.402886  [   32/    6] acc: 0.8125\n",
      "loss: 0.450618  [   32/    6] acc: 0.8125\n",
      "loss: 0.693221  [   32/    6] acc: 0.78125\n",
      "loss: 0.435352  [   32/    6] acc: 0.78125\n",
      "loss: 0.375609  [   32/    6] acc: 0.8125\n"
     ]
    }
   ],
   "source": [
    "size = len(train_dataloader)\n",
    "model.train()\n",
    "epochs = 100\n",
    "for t in range(epochs):\n",
    "    for batch, (X, y) in enumerate(train_dataloader):\n",
    "        X, y = X.to(device, dtype = torch.float), y.to(device)        \n",
    "\n",
    "        # Compute prediction error\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "        \n",
    "        # accuracy\n",
    "        correct = (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "        acc = correct / y.shape[0]\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), (batch + 1) * len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}] acc: {acc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "5b992a8a-cf59-4f20-b001-a9d5247bcd9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = []\n",
    "true = []\n",
    "for batch, (X, y) in enumerate(test_dataloader):\n",
    "    X, y = X.to(device, dtype = torch.float), y.to(device)\n",
    "    \n",
    "    pred.append(model(X).argmax(1).item())\n",
    "    true.append(y.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "e5b0ca31-8762-4359-8d7f-5463b86eda48",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "176\n",
      "176\n"
     ]
    }
   ],
   "source": [
    "print(len(pred))\n",
    "print(len(true))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "b7be7097-809a-4a02-af53-89ceab952f0c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "      Normal       0.54      0.52      0.53        56\n",
      "    Abnormal       0.78      0.79      0.79       120\n",
      "\n",
      "    accuracy                           0.70       176\n",
      "   macro avg       0.66      0.65      0.66       176\n",
      "weighted avg       0.70      0.70      0.70       176\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(classification_report(true, pred, target_names=[\"Normal\", \"Abnormal\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d497e76-8dc4-4499-a60b-2bc380bba014",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
