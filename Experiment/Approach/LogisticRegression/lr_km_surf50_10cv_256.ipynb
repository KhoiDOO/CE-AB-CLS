{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "230df03c",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "85b769aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from random import randint\n",
    "import math\n",
    "from datetime import datetime\n",
    "import pickle\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c8334f40",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'D:\\\\Github\\\\Stomach-Status-Classification\\\\Experiment\\\\Approach\\\\LogisticRegression'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c40a59e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(3):\n",
    "    os.chdir(\"..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e7a47c02",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'D:\\\\Github\\\\Stomach-Status-Classification'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c33c199b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['D:\\\\Github\\\\Stomach-Status-Classification\\\\Data set\\\\kmean_dataset\\\\10cv_128\\\\surf_test_50_4_3_True_Falsehisto_128_cv_index0.csv', 'D:\\\\Github\\\\Stomach-Status-Classification\\\\Data set\\\\kmean_dataset\\\\10cv_128\\\\surf_test_50_4_3_True_Falsehisto_128_cv_index1.csv', 'D:\\\\Github\\\\Stomach-Status-Classification\\\\Data set\\\\kmean_dataset\\\\10cv_128\\\\surf_test_50_4_3_True_Falsehisto_128_cv_index2.csv', 'D:\\\\Github\\\\Stomach-Status-Classification\\\\Data set\\\\kmean_dataset\\\\10cv_128\\\\surf_test_50_4_3_True_Falsehisto_128_cv_index3.csv', 'D:\\\\Github\\\\Stomach-Status-Classification\\\\Data set\\\\kmean_dataset\\\\10cv_128\\\\surf_test_50_4_3_True_Falsehisto_128_cv_index4.csv', 'D:\\\\Github\\\\Stomach-Status-Classification\\\\Data set\\\\kmean_dataset\\\\10cv_128\\\\surf_test_50_4_3_True_Falsehisto_128_cv_index5.csv', 'D:\\\\Github\\\\Stomach-Status-Classification\\\\Data set\\\\kmean_dataset\\\\10cv_128\\\\surf_test_50_4_3_True_Falsehisto_128_cv_index6.csv', 'D:\\\\Github\\\\Stomach-Status-Classification\\\\Data set\\\\kmean_dataset\\\\10cv_128\\\\surf_test_50_4_3_True_Falsehisto_128_cv_index7.csv', 'D:\\\\Github\\\\Stomach-Status-Classification\\\\Data set\\\\kmean_dataset\\\\10cv_128\\\\surf_test_50_4_3_True_Falsehisto_128_cv_index8.csv', 'D:\\\\Github\\\\Stomach-Status-Classification\\\\Data set\\\\kmean_dataset\\\\10cv_128\\\\surf_test_50_4_3_True_Falsehisto_128_cv_index9.csv', 'D:\\\\Github\\\\Stomach-Status-Classification\\\\Data set\\\\kmean_dataset\\\\10cv_128\\\\surf_train_50_4_3_True_Falsehisto_128_cv_index0.csv', 'D:\\\\Github\\\\Stomach-Status-Classification\\\\Data set\\\\kmean_dataset\\\\10cv_128\\\\surf_train_50_4_3_True_Falsehisto_128_cv_index1.csv', 'D:\\\\Github\\\\Stomach-Status-Classification\\\\Data set\\\\kmean_dataset\\\\10cv_128\\\\surf_train_50_4_3_True_Falsehisto_128_cv_index2.csv', 'D:\\\\Github\\\\Stomach-Status-Classification\\\\Data set\\\\kmean_dataset\\\\10cv_128\\\\surf_train_50_4_3_True_Falsehisto_128_cv_index3.csv', 'D:\\\\Github\\\\Stomach-Status-Classification\\\\Data set\\\\kmean_dataset\\\\10cv_128\\\\surf_train_50_4_3_True_Falsehisto_128_cv_index4.csv', 'D:\\\\Github\\\\Stomach-Status-Classification\\\\Data set\\\\kmean_dataset\\\\10cv_128\\\\surf_train_50_4_3_True_Falsehisto_128_cv_index5.csv', 'D:\\\\Github\\\\Stomach-Status-Classification\\\\Data set\\\\kmean_dataset\\\\10cv_128\\\\surf_train_50_4_3_True_Falsehisto_128_cv_index6.csv', 'D:\\\\Github\\\\Stomach-Status-Classification\\\\Data set\\\\kmean_dataset\\\\10cv_128\\\\surf_train_50_4_3_True_Falsehisto_128_cv_index7.csv', 'D:\\\\Github\\\\Stomach-Status-Classification\\\\Data set\\\\kmean_dataset\\\\10cv_128\\\\surf_train_50_4_3_True_Falsehisto_128_cv_index8.csv', 'D:\\\\Github\\\\Stomach-Status-Classification\\\\Data set\\\\kmean_dataset\\\\10cv_128\\\\surf_train_50_4_3_True_Falsehisto_128_cv_index9.csv']\n"
     ]
    }
   ],
   "source": [
    "main_data_dir = os.getcwd() + \"\\\\Data set\"\n",
    "kmean_data_dir = main_data_dir + \"\\\\kmean_dataset\"\n",
    "kmean_data_10cv_dir = kmean_data_dir + \"\\\\10cv_128\"\n",
    "import glob\n",
    "\n",
    "kmean_data_10cv_lst = [x for x in glob.glob(kmean_data_10cv_dir + '\\\\*') if 'surf_train_50' in x or 'surf_test_50' in x and 'index' in x]\n",
    "print(kmean_data_10cv_lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "580af549",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['D:\\\\Github\\\\Stomach-Status-Classification\\\\Data set\\\\kmean_dataset\\\\10cv_128\\\\surf_train_50_4_3_True_Falsehisto_128_cv_index0.csv', 'D:\\\\Github\\\\Stomach-Status-Classification\\\\Data set\\\\kmean_dataset\\\\10cv_128\\\\surf_train_50_4_3_True_Falsehisto_128_cv_index1.csv', 'D:\\\\Github\\\\Stomach-Status-Classification\\\\Data set\\\\kmean_dataset\\\\10cv_128\\\\surf_train_50_4_3_True_Falsehisto_128_cv_index2.csv', 'D:\\\\Github\\\\Stomach-Status-Classification\\\\Data set\\\\kmean_dataset\\\\10cv_128\\\\surf_train_50_4_3_True_Falsehisto_128_cv_index3.csv', 'D:\\\\Github\\\\Stomach-Status-Classification\\\\Data set\\\\kmean_dataset\\\\10cv_128\\\\surf_train_50_4_3_True_Falsehisto_128_cv_index4.csv', 'D:\\\\Github\\\\Stomach-Status-Classification\\\\Data set\\\\kmean_dataset\\\\10cv_128\\\\surf_train_50_4_3_True_Falsehisto_128_cv_index5.csv', 'D:\\\\Github\\\\Stomach-Status-Classification\\\\Data set\\\\kmean_dataset\\\\10cv_128\\\\surf_train_50_4_3_True_Falsehisto_128_cv_index6.csv', 'D:\\\\Github\\\\Stomach-Status-Classification\\\\Data set\\\\kmean_dataset\\\\10cv_128\\\\surf_train_50_4_3_True_Falsehisto_128_cv_index7.csv', 'D:\\\\Github\\\\Stomach-Status-Classification\\\\Data set\\\\kmean_dataset\\\\10cv_128\\\\surf_train_50_4_3_True_Falsehisto_128_cv_index8.csv', 'D:\\\\Github\\\\Stomach-Status-Classification\\\\Data set\\\\kmean_dataset\\\\10cv_128\\\\surf_train_50_4_3_True_Falsehisto_128_cv_index9.csv']\n",
      "\n",
      "['D:\\\\Github\\\\Stomach-Status-Classification\\\\Data set\\\\kmean_dataset\\\\10cv_128\\\\surf_test_50_4_3_True_Falsehisto_128_cv_index0.csv', 'D:\\\\Github\\\\Stomach-Status-Classification\\\\Data set\\\\kmean_dataset\\\\10cv_128\\\\surf_test_50_4_3_True_Falsehisto_128_cv_index1.csv', 'D:\\\\Github\\\\Stomach-Status-Classification\\\\Data set\\\\kmean_dataset\\\\10cv_128\\\\surf_test_50_4_3_True_Falsehisto_128_cv_index2.csv', 'D:\\\\Github\\\\Stomach-Status-Classification\\\\Data set\\\\kmean_dataset\\\\10cv_128\\\\surf_test_50_4_3_True_Falsehisto_128_cv_index3.csv', 'D:\\\\Github\\\\Stomach-Status-Classification\\\\Data set\\\\kmean_dataset\\\\10cv_128\\\\surf_test_50_4_3_True_Falsehisto_128_cv_index4.csv', 'D:\\\\Github\\\\Stomach-Status-Classification\\\\Data set\\\\kmean_dataset\\\\10cv_128\\\\surf_test_50_4_3_True_Falsehisto_128_cv_index5.csv', 'D:\\\\Github\\\\Stomach-Status-Classification\\\\Data set\\\\kmean_dataset\\\\10cv_128\\\\surf_test_50_4_3_True_Falsehisto_128_cv_index6.csv', 'D:\\\\Github\\\\Stomach-Status-Classification\\\\Data set\\\\kmean_dataset\\\\10cv_128\\\\surf_test_50_4_3_True_Falsehisto_128_cv_index7.csv', 'D:\\\\Github\\\\Stomach-Status-Classification\\\\Data set\\\\kmean_dataset\\\\10cv_128\\\\surf_test_50_4_3_True_Falsehisto_128_cv_index8.csv', 'D:\\\\Github\\\\Stomach-Status-Classification\\\\Data set\\\\kmean_dataset\\\\10cv_128\\\\surf_test_50_4_3_True_Falsehisto_128_cv_index9.csv']\n",
      "10\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "kmean_train_paths = [x for x in kmean_data_10cv_lst if 'train' in x]\n",
    "kmean_test_paths = [x for x in kmean_data_10cv_lst if 'test' in x]\n",
    "print(kmean_train_paths)\n",
    "print()\n",
    "print(kmean_test_paths)\n",
    "print(len(kmean_train_paths))\n",
    "print(len(kmean_test_paths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0a580aa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_categorical(y, num_classes=None, dtype='float32'):\n",
    "    \"\"\"to_categorical _summary_\n",
    "\n",
    "    Arguments:\n",
    "        y -- The label of the data set with the shape of [None, 1]\n",
    "\n",
    "    Keyword Arguments:\n",
    "        num_classes -- The num_classes in the data set (default: {None})\n",
    "        dtype -- the type of each element of the label after reshape (default: {'float32'})\n",
    "\n",
    "    Returns:\n",
    "        the label of the data set with the shape of [number of samples, number of classes].\n",
    "    \"\"\"    \n",
    "    y = np.array(y, dtype='int')\n",
    "    input_shape = y.shape\n",
    "    if input_shape and input_shape[-1] == 1 and len(input_shape) > 1:\n",
    "        input_shape = tuple(input_shape[:-1])\n",
    "    y = y.ravel()\n",
    "    if not num_classes:\n",
    "        num_classes = np.max(y) + 1\n",
    "    n = y.shape[0]\n",
    "    categorical = np.zeros((n, num_classes), dtype=dtype)\n",
    "    categorical[np.arange(n), y] = 1\n",
    "    output_shape = input_shape + (num_classes,)\n",
    "    categorical = np.reshape(categorical, output_shape)\n",
    "    return categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "60972d8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class roc_curve_score:\n",
    "    def __init__(self, y_true, y_pred, num_class):\n",
    "        self.y_test = to_categorical(y_true)\n",
    "        self.y_score = to_categorical(y_pred)\n",
    "        print(np.unique(self.y_test), np.unique(self.y_score))\n",
    "        self.fprs = {}\n",
    "        self.tprs = {}\n",
    "        self.thresh_holds = {}\n",
    "        for x in range(num_class):\n",
    "            self.fprs[x], self.tprs[x], self.thresh_holds[x] = roc_curve(self.y_test[:, x], self.y_score[:, x], drop_intermediate=False)\n",
    "            self.fprs[x] = self.fprs[x].tolist()\n",
    "            self.tprs[x] = self.tprs[x].tolist()\n",
    "            self.thresh_holds[x] = self.thresh_holds[x].tolist()\n",
    "        \n",
    "        self.fpr_micro_avg, self.tpr_micro_avg, self.threshold_micro_avg, = roc_curve(self.y_test.ravel(), self.y_score.ravel())\n",
    "        \n",
    "        all_fpr = np.unique(np.concatenate([self.fprs[i] for i in range(num_class)]))\n",
    "        mean_tpr = np.zeros_like(all_fpr)\n",
    "        for i in range(num_class):\n",
    "            mean_tpr += np.interp(all_fpr, self.fprs[i], self.tprs[i])\n",
    "        mean_tpr /= num_class\n",
    "        self.fpr_macro_avg = all_fpr.tolist()\n",
    "        self.tpr_macro_avg = mean_tpr.tolist()\n",
    "    \n",
    "    def get_tpr(self, _class = None):\n",
    "        if(_class):\n",
    "            return self.tprs[_class]\n",
    "        else:\n",
    "            return self.tprs\n",
    "    \n",
    "    def get_fpr(self, _class = None):\n",
    "        if(_class):\n",
    "            return self.fprs[_class]\n",
    "        else:\n",
    "            return self.fprs\n",
    "\n",
    "    def get_thresholds(self, _class = None):\n",
    "        if(_class):\n",
    "            return self.thresh_holds[_class]\n",
    "        else:\n",
    "            return self.thresh_holds\n",
    "\n",
    "    def get_roc_dict(self):\n",
    "        return {\n",
    "            \"tpr\" : self.get_tpr(),\n",
    "            \"fpr\" : self.get_fpr(),\n",
    "            \"thresholds\" : self.get_thresholds(),\n",
    "            \"fpr_micro_avg\" : self.fpr_micro_avg.tolist(),\n",
    "            \"tpr_micro_avg\" : self.tpr_micro_avg.tolist(),\n",
    "            \"fpr_macro_avg\" : self.fpr_macro_avg,\n",
    "            \"tpr_macro_avg\" : self.tpr_macro_avg,\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2089c5bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Confusion Matrix': {'TP': 43,\n",
       "  'TN': 55,\n",
       "  'FN': 52,\n",
       "  'FP': 52,\n",
       "  'precision': 0.45263157894736844,\n",
       "  'recall': 0.45263157894736844,\n",
       "  'f1_score': 0.45263157894736844,\n",
       "  'sensitivity': 0.45263157894736844,\n",
       "  'specificity': 0.514018691588785,\n",
       "  'negative_predictive_value': 0.514018691588785,\n",
       "  'false_negative_rate': 0.5473684210526316,\n",
       "  'false_positive_rate': 0.48598130841121495,\n",
       "  'false_discovery_rate': 0.5473684210526316,\n",
       "  'false_omission_rate': 0.48598130841121495,\n",
       "  'Positive_likelihood_ratio': 0.9313765182186236,\n",
       "  'Negative_likelihood_ratio': 1.0648803827751199,\n",
       "  'prevalence_threshold': 0.5088855219471327,\n",
       "  'threat_score': 0.27044025157232704,\n",
       "  'Prevalence': 0.47029702970297027,\n",
       "  'Matthews_correlation_coefficient': -3.280839101214612e-06,\n",
       "  'Fowlkes_Mallows_index': 0.9514531821875089,\n",
       "  'informedness': -0.03334972946384651,\n",
       "  'markedness': -0.03334972946384651,\n",
       "  'Diagnostic_odds_ratio': 0.8746301775147928,\n",
       "  'accuracy': 0.48514851485148514,\n",
       "  'balanced_accuracy': 0.48332513526807674},\n",
       " 'ROC_AUC_SCORE': {'Macro': 0.48819214149331724,\n",
       "  'Micro': 0.48819214149331724,\n",
       "  'Weight': 0.48819214149331724},\n",
       " 'Classification Report': {'0': {'precision': 0.45263157894736844,\n",
       "   'recall': 0.46236559139784944,\n",
       "   'f1-score': 0.4574468085106383,\n",
       "   'support': 93},\n",
       "  '1': {'precision': 0.5238095238095238,\n",
       "   'recall': 0.514018691588785,\n",
       "   'f1-score': 0.5188679245283019,\n",
       "   'support': 107},\n",
       "  'accuracy': 0.49,\n",
       "  'macro avg': {'precision': 0.4882205513784461,\n",
       "   'recall': 0.48819214149331724,\n",
       "   'f1-score': 0.48815736651947006,\n",
       "   'support': 200},\n",
       "  'weighted avg': {'precision': 0.49071177944862154,\n",
       "   'recall': 0.49,\n",
       "   'f1-score': 0.49030710558008833,\n",
       "   'support': 200}}}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class utils:\n",
    "    def __init__(self, confusion_matrix, y_true, y_pred):\n",
    "        \"\"\"\n",
    "            - confusion_matrix: 2x2 numpy array\n",
    "            - y_true: array of label\n",
    "            - y_pred: array of output value calculated by model\n",
    "            - fold_count: number of folds\n",
    "        \"\"\"\n",
    "        \n",
    "        # Initilize all indicator\n",
    "        self.TP = confusion_matrix[0][0] # true positive\n",
    "        self.FN = confusion_matrix[1][0] # false negative\n",
    "        self.FP = confusion_matrix[1][0] # false positive\n",
    "        self.TN = confusion_matrix[1][1] # true negative\n",
    "        self.precision = self.TP/(self.FN +  self.TP) # Precision Score - Positive Predictive Value\n",
    "        self.recall = self.TP / (self.TP + self.FN)\n",
    "        self.f1_score =  (2 * self.TP) / (2 * self.TP + self.FP + self.FN)\n",
    "        self.sensitivity = self.TP / (self.TP + self.FN) # True Positive Rate\n",
    "        self.specificity = self.TN / (self.TN + self.FP) # False Positive Rate\n",
    "        self.negative_predictive_value = self.TN / (self.TN + self.FN) # Negative Predictive Value\n",
    "        self.false_negative_rate = self.FN / (self.FN + self.TP) # False Negative Rate\n",
    "        self.false_positive_rate = self.FP / (self.FP + self.TN) # False Positive Rate\n",
    "        self.false_discovery_rate = self.FP / (self.FP + self.TP) # False Discovery Rate\n",
    "        self.false_omission_rate = self.FN / (self.FN + self.TN) # False Ommision Rate\n",
    "        self.positive_likelihood_ratio = self.sensitivity / self.false_positive_rate # Positive Likelihood Ratio\n",
    "        self.negative_likelihood_ratio = self.false_negative_rate / self.specificity # Negative Likelihood Ratio\n",
    "        self.prevalence_threshold = math.sqrt(self.false_positive_rate) / (math.sqrt(self.sensitivity) + \n",
    "                                                                      math.sqrt(self.false_positive_rate)) # Prevalance Threshold\n",
    "        self.threat_score = self.TP / (self.TN + self.FN + self.FP) # Threat Score\n",
    "        self.prevalence = (self.TP + self.FN)/(self.TP + self.FN + self.TN + self.FP) # Prevalance \n",
    "        #  Matthews correlation coefficient\n",
    "        self.matthews_correlation_coefficient = (self.TP*self.TN - self.FN*self.FP) / ((self.TP + self.FP)\n",
    "                                                                                       *(self.TP + self.FN)\n",
    "                                                                                       *(self.TN + self.FP)\n",
    "                                                                                       *(self.TN + self.FN))\n",
    "        self.fowlkes_mallows_index = math.sqrt(self.sensitivity + self.precision) # Fowlkes–Mallows index\n",
    "        self.informedness = self.sensitivity + self.specificity - 1 # informedness\n",
    "        self.markedness = self.precision + self.negative_predictive_value - 1 # markedness\n",
    "        self.diagnostic_odds_ratio = self.positive_likelihood_ratio / self.negative_likelihood_ratio # Diagnostic odds ratio\n",
    "        self.accuracy = (self.TP + self.TN) / (self.TP + self.TN + self.FP + self.FN)\n",
    "        self.balanced_accuracy = (self.sensitivity + self.specificity) / 2\n",
    "        self.roc_auc_macro = roc_auc_score(y_true, y_pred)\n",
    "        self.roc_auc_micro = roc_auc_score(y_true, y_pred, average = 'micro')\n",
    "        self.roc_auc_weighted = roc_auc_score(y_true, y_pred, average = 'weighted')\n",
    "        self.cls_report = classification_report(y_true, y_pred)\n",
    "        \n",
    "        # Initilize the structure of output_dicts\n",
    "        self.confusion_matrix = {\n",
    "            \"TP\" : self.TP,\n",
    "            \"TN\" : self.TN,\n",
    "            \"FN\" : self.FN,\n",
    "            \"FP\" : self.FP,\n",
    "            \"precision\" : self.precision,\n",
    "            \"recall\" : self.recall,\n",
    "            \"f1_score\" : self.f1_score,\n",
    "            \"sensitivity\" : self.sensitivity,\n",
    "            \"specificity\" : self.specificity,\n",
    "            \"negative_predictive_value\" : self.negative_predictive_value,\n",
    "            \"false_negative_rate\" : self.false_negative_rate,\n",
    "            \"false_positive_rate\" : self.false_positive_rate,\n",
    "            \"false_discovery_rate\" : self.false_discovery_rate,\n",
    "            \"false_omission_rate\" : self.false_omission_rate,\n",
    "            \"Positive_likelihood_ratio\" : self.positive_likelihood_ratio,\n",
    "            \"Negative_likelihood_ratio\" : self.negative_likelihood_ratio,\n",
    "            \"prevalence_threshold\" : self.prevalence_threshold,\n",
    "            \"threat_score\" : self.threat_score,\n",
    "            \"Prevalence\" : self.prevalence,\n",
    "            \"Matthews_correlation_coefficient\" : self.matthews_correlation_coefficient,\n",
    "            \"Fowlkes_Mallows_index\" : self.fowlkes_mallows_index,\n",
    "            \"informedness\" : self.informedness,\n",
    "            \"markedness\" : self.markedness,\n",
    "            \"Diagnostic_odds_ratio\" : self.diagnostic_odds_ratio,\n",
    "            \"accuracy\" : self.accuracy,\n",
    "            \"balanced_accuracy\" : self.balanced_accuracy\n",
    "        }\n",
    "        \n",
    "        self.roc_auc_score = {\n",
    "            \"Macro\": self.roc_auc_macro, \n",
    "            \"Micro\": self.roc_auc_micro,\n",
    "            \"Weight\": self.roc_auc_weighted\n",
    "        }\n",
    "        \n",
    "#         self.roc_curve = roc_curve_score(y_true=y_true, y_pred=y_pred, num_class=2).get_roc_dict()\n",
    "        \n",
    "        self.sub_dict = {\n",
    "            \"Confusion Matrix\" : self.confusion_matrix,\n",
    "            \"ROC_AUC_SCORE\" : self.roc_auc_score,\n",
    "            \"Classification Report\" : classification_report(y_true.tolist(), y_pred.tolist(), \n",
    "                                                            labels = [0, 1], # 0 : Licit, 1 : Illicit\n",
    "                                                            output_dict = True),\n",
    "#             \"ROC_DRAW\" : self.roc_curve\n",
    "        }\n",
    "    def get_value(self):\n",
    "        return self.sub_dict\n",
    "        \n",
    "# Test \n",
    "y_true = np.array([randint(0,1) for x in range(200)])\n",
    "y_pred = np.array([randint(0,1) for x in range(200)])\n",
    "confusion_matrix_test = confusion_matrix(y_true, y_pred)\n",
    "base_utils = utils(confusion_matrix_test, y_true, y_pred)\n",
    "base_utils.get_value()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "98239c4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Training:\n",
    "    def __init__(self, fold_count, X, y):\n",
    "        self.fold_count = fold_count\n",
    "        self.param_grid = {\n",
    "            \"penalty\" : [\"l2\"],\n",
    "            \"fit_intercept\" : [True, False],\n",
    "            \"class_weight\" : [\"balanced\", None],\n",
    "            \"solver\" : [\"newton-cg\", \"lbfgs\", \"liblinear\", \"sag\", \"saga\"]\n",
    "        }\n",
    "        self.kf = KFold(n_splits=fold_count)\n",
    "        self.history = {}\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "    \n",
    "    def get_fold_value(self):\n",
    "        return self.kf        \n",
    "        \n",
    "    def training(self, penalty, \n",
    "                 fit_intercept, \n",
    "                 class_weight, \n",
    "                 solver, train_case):\n",
    "    \n",
    "        # output_dict initilize\n",
    "        output_dict = {}\n",
    "    \n",
    "        # poiter track the index of fold\n",
    "        fold_index = 0\n",
    "    \n",
    "        for train_index, test_index in zip(self.X, self.y):\n",
    "            train_df = pd.read_csv(train_index)\n",
    "            test_df = pd.read_csv(test_index)\n",
    "            \n",
    "            print(\"\\tFold: {}\".format(fold_index))\n",
    "            print(\"\\tTRAIN:\", train_index, \"\\n\\tTEST:\", test_index)\n",
    "        \n",
    "            # folding data\n",
    "            x_train, y_train = train_df.iloc[:, 2:-1], train_df.iloc[:, -1]\n",
    "            x_test, y_test = test_df.iloc[:, 2:-1], test_df.iloc[:, -1]\n",
    "    \n",
    "            # Training\n",
    "            print(\"\\t\\tTraining : {}\".format(fold_index), end = \" -- \")\n",
    "            print(\"Start: {}\".format(datetime.now().strftime(\"%m/%d/%Y, %H:%M:%S\")), end=\" --- \")\n",
    "            model = LogisticRegression(penalty = penalty,\n",
    "                                         fit_intercept = fit_intercept,\n",
    "                                         class_weight = class_weight,\n",
    "                                         solver = solver)\n",
    "            model.fit(x_train,y_train)\n",
    "            print(\"End: {}\".format(datetime.now().strftime(\"%m/%d/%Y, %H:%M:%S\")))\n",
    "        \n",
    "            # Testing\n",
    "            print(\"\\t\\tValidation: {}\".format(fold_index), end = \" -- \")\n",
    "            print(\"Start: {}\".format(datetime.now().strftime(\"%m/%d/%Y, %H:%M:%S\")), end=\"---\")\n",
    "            y_pred = model.predict(x_test)\n",
    "            print(\"End: {}\".format(datetime.now().strftime(\"%m/%d/%Y, %H:%M:%S\")))\n",
    "        \n",
    "            # Evaluation\n",
    "            cm = confusion_matrix(y_test, y_pred)\n",
    "            current_utils = utils(cm, y_test, y_pred)\n",
    "            output_dict[\"fold_{}\".format(fold_index)] = current_utils.get_value()\n",
    "            \n",
    "            fold_index += 1\n",
    "        print(\"\\n\")\n",
    "        return output_dict \n",
    "    \n",
    "    def train(self, path):\n",
    "        count = 0\n",
    "        for x in self.param_grid[\"penalty\"]:\n",
    "            for i in self.param_grid[\"fit_intercept\"]:\n",
    "                for j in self.param_grid[\"class_weight\"]:\n",
    "                    for k in self.param_grid[\"solver\"]:\n",
    "                        print(\"Traning Case: {}\".format(count))\n",
    "                        self.history[\"train_{}\".format(count)] = {\n",
    "                            \"param\" : {\n",
    "                            \"penalty\" : x,\n",
    "                            \"fit_intercept\" : i,\n",
    "                            \"class_weight\" : j,\n",
    "                            \"solver\" : k,\n",
    "                            },\n",
    "                            \"train_fold\" : self.training(x, i, j, k, count)\n",
    "                        }\n",
    "                        count += 1\n",
    "        with open(path.format(self.fold_count), 'wb') as f:\n",
    "            pickle.dump(self.history, f)\n",
    "        \n",
    "        return self.history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccfbd61c",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d78a7a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traning Case: 0\n",
      "\tFold: 0\n",
      "\tTRAIN: D:\\Github\\Stomach-Status-Classification\\Data set\\kmean_dataset\\10cv_128\\surf_train_50_4_3_True_Falsehisto_128_cv_index0.csv \n",
      "\tTEST: D:\\Github\\Stomach-Status-Classification\\Data set\\kmean_dataset\\10cv_128\\surf_test_50_4_3_True_Falsehisto_128_cv_index0.csv\n",
      "\t\tTraining : 0 -- Start: 12/12/2022, 17:23:51 --- End: 12/12/2022, 17:23:51\n",
      "\t\tValidation: 0 -- Start: 12/12/2022, 17:23:51---End: 12/12/2022, 17:23:51\n",
      "\tFold: 1\n",
      "\tTRAIN: D:\\Github\\Stomach-Status-Classification\\Data set\\kmean_dataset\\10cv_128\\surf_train_50_4_3_True_Falsehisto_128_cv_index1.csv \n",
      "\tTEST: D:\\Github\\Stomach-Status-Classification\\Data set\\kmean_dataset\\10cv_128\\surf_test_50_4_3_True_Falsehisto_128_cv_index1.csv\n",
      "\t\tTraining : 1 -- Start: 12/12/2022, 17:23:51 --- End: 12/12/2022, 17:23:51\n",
      "\t\tValidation: 1 -- Start: 12/12/2022, 17:23:51---End: 12/12/2022, 17:23:51\n",
      "\tFold: 2\n",
      "\tTRAIN: D:\\Github\\Stomach-Status-Classification\\Data set\\kmean_dataset\\10cv_128\\surf_train_50_4_3_True_Falsehisto_128_cv_index2.csv \n",
      "\tTEST: D:\\Github\\Stomach-Status-Classification\\Data set\\kmean_dataset\\10cv_128\\surf_test_50_4_3_True_Falsehisto_128_cv_index2.csv\n",
      "\t\tTraining : 2 -- Start: 12/12/2022, 17:23:51 --- End: 12/12/2022, 17:23:51\n",
      "\t\tValidation: 2 -- Start: 12/12/2022, 17:23:51---End: 12/12/2022, 17:23:51\n",
      "\tFold: 3\n",
      "\tTRAIN: D:\\Github\\Stomach-Status-Classification\\Data set\\kmean_dataset\\10cv_128\\surf_train_50_4_3_True_Falsehisto_128_cv_index3.csv \n",
      "\tTEST: D:\\Github\\Stomach-Status-Classification\\Data set\\kmean_dataset\\10cv_128\\surf_test_50_4_3_True_Falsehisto_128_cv_index3.csv\n",
      "\t\tTraining : 3 -- Start: 12/12/2022, 17:23:52 --- End: 12/12/2022, 17:23:52\n",
      "\t\tValidation: 3 -- Start: 12/12/2022, 17:23:52---End: 12/12/2022, 17:23:52\n",
      "\tFold: 4\n",
      "\tTRAIN: D:\\Github\\Stomach-Status-Classification\\Data set\\kmean_dataset\\10cv_128\\surf_train_50_4_3_True_Falsehisto_128_cv_index4.csv \n",
      "\tTEST: D:\\Github\\Stomach-Status-Classification\\Data set\\kmean_dataset\\10cv_128\\surf_test_50_4_3_True_Falsehisto_128_cv_index4.csv\n",
      "\t\tTraining : 4 -- Start: 12/12/2022, 17:23:52 --- End: 12/12/2022, 17:23:52\n",
      "\t\tValidation: 4 -- Start: 12/12/2022, 17:23:52---End: 12/12/2022, 17:23:52\n",
      "\tFold: 5\n",
      "\tTRAIN: D:\\Github\\Stomach-Status-Classification\\Data set\\kmean_dataset\\10cv_128\\surf_train_50_4_3_True_Falsehisto_128_cv_index5.csv \n",
      "\tTEST: D:\\Github\\Stomach-Status-Classification\\Data set\\kmean_dataset\\10cv_128\\surf_test_50_4_3_True_Falsehisto_128_cv_index5.csv\n",
      "\t\tTraining : 5 -- Start: 12/12/2022, 17:23:52 --- End: 12/12/2022, 17:23:52\n",
      "\t\tValidation: 5 -- Start: 12/12/2022, 17:23:52---End: 12/12/2022, 17:23:52\n",
      "\tFold: 6\n",
      "\tTRAIN: D:\\Github\\Stomach-Status-Classification\\Data set\\kmean_dataset\\10cv_128\\surf_train_50_4_3_True_Falsehisto_128_cv_index6.csv \n",
      "\tTEST: D:\\Github\\Stomach-Status-Classification\\Data set\\kmean_dataset\\10cv_128\\surf_test_50_4_3_True_Falsehisto_128_cv_index6.csv\n",
      "\t\tTraining : 6 -- Start: 12/12/2022, 17:23:52 --- End: 12/12/2022, 17:23:52\n",
      "\t\tValidation: 6 -- Start: 12/12/2022, 17:23:52---End: 12/12/2022, 17:23:52\n",
      "\tFold: 7\n",
      "\tTRAIN: D:\\Github\\Stomach-Status-Classification\\Data set\\kmean_dataset\\10cv_128\\surf_train_50_4_3_True_Falsehisto_128_cv_index7.csv \n",
      "\tTEST: D:\\Github\\Stomach-Status-Classification\\Data set\\kmean_dataset\\10cv_128\\surf_test_50_4_3_True_Falsehisto_128_cv_index7.csv\n",
      "\t\tTraining : 7 -- Start: 12/12/2022, 17:23:52 --- End: 12/12/2022, 17:23:52\n",
      "\t\tValidation: 7 -- Start: 12/12/2022, 17:23:52---End: 12/12/2022, 17:23:52\n",
      "\tFold: 8\n",
      "\tTRAIN: D:\\Github\\Stomach-Status-Classification\\Data set\\kmean_dataset\\10cv_128\\surf_train_50_4_3_True_Falsehisto_128_cv_index8.csv \n",
      "\tTEST: D:\\Github\\Stomach-Status-Classification\\Data set\\kmean_dataset\\10cv_128\\surf_test_50_4_3_True_Falsehisto_128_cv_index8.csv\n",
      "\t\tTraining : 8 -- Start: 12/12/2022, 17:23:52 --- End: 12/12/2022, 17:23:52\n",
      "\t\tValidation: 8 -- Start: 12/12/2022, 17:23:52---End: 12/12/2022, 17:23:52\n",
      "\tFold: 9\n",
      "\tTRAIN: D:\\Github\\Stomach-Status-Classification\\Data set\\kmean_dataset\\10cv_128\\surf_train_50_4_3_True_Falsehisto_128_cv_index9.csv \n",
      "\tTEST: D:\\Github\\Stomach-Status-Classification\\Data set\\kmean_dataset\\10cv_128\\surf_test_50_4_3_True_Falsehisto_128_cv_index9.csv\n",
      "\t\tTraining : 9 -- Start: 12/12/2022, 17:23:52 --- End: 12/12/2022, 17:23:52\n",
      "\t\tValidation: 9 -- Start: 12/12/2022, 17:23:52---End: 12/12/2022, 17:23:52\n",
      "\n",
      "\n",
      "Traning Case: 1\n",
      "\tFold: 0\n",
      "\tTRAIN: D:\\Github\\Stomach-Status-Classification\\Data set\\kmean_dataset\\10cv_128\\surf_train_50_4_3_True_Falsehisto_128_cv_index0.csv \n",
      "\tTEST: D:\\Github\\Stomach-Status-Classification\\Data set\\kmean_dataset\\10cv_128\\surf_test_50_4_3_True_Falsehisto_128_cv_index0.csv\n",
      "\t\tTraining : 0 -- Start: 12/12/2022, 17:23:52 --- End: 12/12/2022, 17:23:52\n",
      "\t\tValidation: 0 -- Start: 12/12/2022, 17:23:52---End: 12/12/2022, 17:23:52\n",
      "\tFold: 1\n",
      "\tTRAIN: D:\\Github\\Stomach-Status-Classification\\Data set\\kmean_dataset\\10cv_128\\surf_train_50_4_3_True_Falsehisto_128_cv_index1.csv \n",
      "\tTEST: D:\\Github\\Stomach-Status-Classification\\Data set\\kmean_dataset\\10cv_128\\surf_test_50_4_3_True_Falsehisto_128_cv_index1.csv\n",
      "\t\tTraining : 1 -- Start: 12/12/2022, 17:23:52 --- End: 12/12/2022, 17:23:52\n",
      "\t\tValidation: 1 -- Start: 12/12/2022, 17:23:52---End: 12/12/2022, 17:23:52\n",
      "\tFold: 2\n",
      "\tTRAIN: D:\\Github\\Stomach-Status-Classification\\Data set\\kmean_dataset\\10cv_128\\surf_train_50_4_3_True_Falsehisto_128_cv_index2.csv \n",
      "\tTEST: D:\\Github\\Stomach-Status-Classification\\Data set\\kmean_dataset\\10cv_128\\surf_test_50_4_3_True_Falsehisto_128_cv_index2.csv\n",
      "\t\tTraining : 2 -- Start: 12/12/2022, 17:23:52 --- End: 12/12/2022, 17:23:52\n",
      "\t\tValidation: 2 -- Start: 12/12/2022, 17:23:52---End: 12/12/2022, 17:23:52\n",
      "\tFold: 3\n",
      "\tTRAIN: D:\\Github\\Stomach-Status-Classification\\Data set\\kmean_dataset\\10cv_128\\surf_train_50_4_3_True_Falsehisto_128_cv_index3.csv \n",
      "\tTEST: D:\\Github\\Stomach-Status-Classification\\Data set\\kmean_dataset\\10cv_128\\surf_test_50_4_3_True_Falsehisto_128_cv_index3.csv\n",
      "\t\tTraining : 3 -- Start: 12/12/2022, 17:23:52 --- End: 12/12/2022, 17:23:52\n",
      "\t\tValidation: 3 -- Start: 12/12/2022, 17:23:52---End: 12/12/2022, 17:23:52\n",
      "\tFold: 4\n",
      "\tTRAIN: D:\\Github\\Stomach-Status-Classification\\Data set\\kmean_dataset\\10cv_128\\surf_train_50_4_3_True_Falsehisto_128_cv_index4.csv \n",
      "\tTEST: D:\\Github\\Stomach-Status-Classification\\Data set\\kmean_dataset\\10cv_128\\surf_test_50_4_3_True_Falsehisto_128_cv_index4.csv\n",
      "\t\tTraining : 4 -- Start: 12/12/2022, 17:23:52 --- End: 12/12/2022, 17:23:52\n",
      "\t\tValidation: 4 -- Start: 12/12/2022, 17:23:52---End: 12/12/2022, 17:23:52\n",
      "\tFold: 5\n",
      "\tTRAIN: D:\\Github\\Stomach-Status-Classification\\Data set\\kmean_dataset\\10cv_128\\surf_train_50_4_3_True_Falsehisto_128_cv_index5.csv \n",
      "\tTEST: D:\\Github\\Stomach-Status-Classification\\Data set\\kmean_dataset\\10cv_128\\surf_test_50_4_3_True_Falsehisto_128_cv_index5.csv\n",
      "\t\tTraining : 5 -- Start: 12/12/2022, 17:23:52 --- End: 12/12/2022, 17:23:52\n",
      "\t\tValidation: 5 -- Start: 12/12/2022, 17:23:52---End: 12/12/2022, 17:23:52\n",
      "\tFold: 6\n",
      "\tTRAIN: D:\\Github\\Stomach-Status-Classification\\Data set\\kmean_dataset\\10cv_128\\surf_train_50_4_3_True_Falsehisto_128_cv_index6.csv \n",
      "\tTEST: D:\\Github\\Stomach-Status-Classification\\Data set\\kmean_dataset\\10cv_128\\surf_test_50_4_3_True_Falsehisto_128_cv_index6.csv\n",
      "\t\tTraining : 6 -- Start: 12/12/2022, 17:23:52 --- End: 12/12/2022, 17:23:52\n",
      "\t\tValidation: 6 -- Start: 12/12/2022, 17:23:52---End: 12/12/2022, 17:23:52\n",
      "\tFold: 7\n",
      "\tTRAIN: D:\\Github\\Stomach-Status-Classification\\Data set\\kmean_dataset\\10cv_128\\surf_train_50_4_3_True_Falsehisto_128_cv_index7.csv \n",
      "\tTEST: D:\\Github\\Stomach-Status-Classification\\Data set\\kmean_dataset\\10cv_128\\surf_test_50_4_3_True_Falsehisto_128_cv_index7.csv\n",
      "\t\tTraining : 7 -- Start: 12/12/2022, 17:23:52 --- End: 12/12/2022, 17:23:52\n",
      "\t\tValidation: 7 -- Start: 12/12/2022, 17:23:52---End: 12/12/2022, 17:23:52\n",
      "\tFold: 8\n",
      "\tTRAIN: D:\\Github\\Stomach-Status-Classification\\Data set\\kmean_dataset\\10cv_128\\surf_train_50_4_3_True_Falsehisto_128_cv_index8.csv \n",
      "\tTEST: D:\\Github\\Stomach-Status-Classification\\Data set\\kmean_dataset\\10cv_128\\surf_test_50_4_3_True_Falsehisto_128_cv_index8.csv\n",
      "\t\tTraining : 8 -- Start: 12/12/2022, 17:23:52 --- End: 12/12/2022, 17:23:52\n",
      "\t\tValidation: 8 -- Start: 12/12/2022, 17:23:52---End: 12/12/2022, 17:23:52\n",
      "\tFold: 9\n",
      "\tTRAIN: D:\\Github\\Stomach-Status-Classification\\Data set\\kmean_dataset\\10cv_128\\surf_train_50_4_3_True_Falsehisto_128_cv_index9.csv \n",
      "\tTEST: D:\\Github\\Stomach-Status-Classification\\Data set\\kmean_dataset\\10cv_128\\surf_test_50_4_3_True_Falsehisto_128_cv_index9.csv\n",
      "\t\tTraining : 9 -- Start: 12/12/2022, 17:23:52 --- End: 12/12/2022, 17:23:52\n",
      "\t\tValidation: 9 -- Start: 12/12/2022, 17:23:52---End: 12/12/2022, 17:23:52\n",
      "\n",
      "\n",
      "Traning Case: 2\n",
      "\tFold: 0\n",
      "\tTRAIN: D:\\Github\\Stomach-Status-Classification\\Data set\\kmean_dataset\\10cv_128\\surf_train_50_4_3_True_Falsehisto_128_cv_index0.csv \n",
      "\tTEST: D:\\Github\\Stomach-Status-Classification\\Data set\\kmean_dataset\\10cv_128\\surf_test_50_4_3_True_Falsehisto_128_cv_index0.csv\n",
      "\t\tTraining : 0 -- Start: 12/12/2022, 17:23:52 --- End: 12/12/2022, 17:23:52\n",
      "\t\tValidation: 0 -- Start: 12/12/2022, 17:23:52---End: 12/12/2022, 17:23:52\n",
      "\tFold: 1\n",
      "\tTRAIN: D:\\Github\\Stomach-Status-Classification\\Data set\\kmean_dataset\\10cv_128\\surf_train_50_4_3_True_Falsehisto_128_cv_index1.csv \n",
      "\tTEST: D:\\Github\\Stomach-Status-Classification\\Data set\\kmean_dataset\\10cv_128\\surf_test_50_4_3_True_Falsehisto_128_cv_index1.csv\n",
      "\t\tTraining : 1 -- Start: 12/12/2022, 17:23:52 --- End: 12/12/2022, 17:23:52\n",
      "\t\tValidation: 1 -- Start: 12/12/2022, 17:23:52---End: 12/12/2022, 17:23:52\n",
      "\tFold: 2\n",
      "\tTRAIN: D:\\Github\\Stomach-Status-Classification\\Data set\\kmean_dataset\\10cv_128\\surf_train_50_4_3_True_Falsehisto_128_cv_index2.csv \n",
      "\tTEST: D:\\Github\\Stomach-Status-Classification\\Data set\\kmean_dataset\\10cv_128\\surf_test_50_4_3_True_Falsehisto_128_cv_index2.csv\n",
      "\t\tTraining : 2 -- Start: 12/12/2022, 17:23:52 --- End: 12/12/2022, 17:23:52\n",
      "\t\tValidation: 2 -- Start: 12/12/2022, 17:23:52---End: 12/12/2022, 17:23:52\n",
      "\tFold: 3\n",
      "\tTRAIN: D:\\Github\\Stomach-Status-Classification\\Data set\\kmean_dataset\\10cv_128\\surf_train_50_4_3_True_Falsehisto_128_cv_index3.csv \n",
      "\tTEST: D:\\Github\\Stomach-Status-Classification\\Data set\\kmean_dataset\\10cv_128\\surf_test_50_4_3_True_Falsehisto_128_cv_index3.csv\n",
      "\t\tTraining : 3 -- Start: 12/12/2022, 17:23:52 --- End: 12/12/2022, 17:23:52\n",
      "\t\tValidation: 3 -- Start: 12/12/2022, 17:23:52---End: 12/12/2022, 17:23:52\n",
      "\tFold: 4\n",
      "\tTRAIN: D:\\Github\\Stomach-Status-Classification\\Data set\\kmean_dataset\\10cv_128\\surf_train_50_4_3_True_Falsehisto_128_cv_index4.csv \n",
      "\tTEST: D:\\Github\\Stomach-Status-Classification\\Data set\\kmean_dataset\\10cv_128\\surf_test_50_4_3_True_Falsehisto_128_cv_index4.csv\n",
      "\t\tTraining : 4 -- Start: 12/12/2022, 17:23:52 --- End: 12/12/2022, 17:23:52\n",
      "\t\tValidation: 4 -- Start: 12/12/2022, 17:23:52---End: 12/12/2022, 17:23:52\n",
      "\tFold: 5\n",
      "\tTRAIN: D:\\Github\\Stomach-Status-Classification\\Data set\\kmean_dataset\\10cv_128\\surf_train_50_4_3_True_Falsehisto_128_cv_index5.csv \n",
      "\tTEST: D:\\Github\\Stomach-Status-Classification\\Data set\\kmean_dataset\\10cv_128\\surf_test_50_4_3_True_Falsehisto_128_cv_index5.csv\n",
      "\t\tTraining : 5 -- Start: 12/12/2022, 17:23:52 --- End: 12/12/2022, 17:23:52\n",
      "\t\tValidation: 5 -- Start: 12/12/2022, 17:23:52---End: 12/12/2022, 17:23:52\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tFold: 6\n",
      "\tTRAIN: D:\\Github\\Stomach-Status-Classification\\Data set\\kmean_dataset\\10cv_128\\surf_train_50_4_3_True_Falsehisto_128_cv_index6.csv \n",
      "\tTEST: D:\\Github\\Stomach-Status-Classification\\Data set\\kmean_dataset\\10cv_128\\surf_test_50_4_3_True_Falsehisto_128_cv_index6.csv\n",
      "\t\tTraining : 6 -- Start: 12/12/2022, 17:23:52 --- End: 12/12/2022, 17:23:52\n",
      "\t\tValidation: 6 -- Start: 12/12/2022, 17:23:52---End: 12/12/2022, 17:23:52\n",
      "\tFold: 7\n",
      "\tTRAIN: D:\\Github\\Stomach-Status-Classification\\Data set\\kmean_dataset\\10cv_128\\surf_train_50_4_3_True_Falsehisto_128_cv_index7.csv \n",
      "\tTEST: D:\\Github\\Stomach-Status-Classification\\Data set\\kmean_dataset\\10cv_128\\surf_test_50_4_3_True_Falsehisto_128_cv_index7.csv\n",
      "\t\tTraining : 7 -- Start: 12/12/2022, 17:23:52 --- End: 12/12/2022, 17:23:52\n",
      "\t\tValidation: 7 -- Start: 12/12/2022, 17:23:52---End: 12/12/2022, 17:23:52\n",
      "\tFold: 8\n",
      "\tTRAIN: D:\\Github\\Stomach-Status-Classification\\Data set\\kmean_dataset\\10cv_128\\surf_train_50_4_3_True_Falsehisto_128_cv_index8.csv \n",
      "\tTEST: D:\\Github\\Stomach-Status-Classification\\Data set\\kmean_dataset\\10cv_128\\surf_test_50_4_3_True_Falsehisto_128_cv_index8.csv\n",
      "\t\tTraining : 8 -- Start: 12/12/2022, 17:23:52 --- End: 12/12/2022, 17:23:52\n",
      "\t\tValidation: 8 -- Start: 12/12/2022, 17:23:52---End: 12/12/2022, 17:23:52\n",
      "\tFold: 9\n",
      "\tTRAIN: D:\\Github\\Stomach-Status-Classification\\Data set\\kmean_dataset\\10cv_128\\surf_train_50_4_3_True_Falsehisto_128_cv_index9.csv \n",
      "\tTEST: D:\\Github\\Stomach-Status-Classification\\Data set\\kmean_dataset\\10cv_128\\surf_test_50_4_3_True_Falsehisto_128_cv_index9.csv\n",
      "\t\tTraining : 9 -- Start: 12/12/2022, 17:23:52 --- End: 12/12/2022, 17:23:52\n",
      "\t\tValidation: 9 -- Start: 12/12/2022, 17:23:52---End: 12/12/2022, 17:23:52\n",
      "\n",
      "\n",
      "Traning Case: 3\n",
      "\tFold: 0\n",
      "\tTRAIN: D:\\Github\\Stomach-Status-Classification\\Data set\\kmean_dataset\\10cv_128\\surf_train_50_4_3_True_Falsehisto_128_cv_index0.csv \n",
      "\tTEST: D:\\Github\\Stomach-Status-Classification\\Data set\\kmean_dataset\\10cv_128\\surf_test_50_4_3_True_Falsehisto_128_cv_index0.csv\n",
      "\t\tTraining : 0 -- Start: 12/12/2022, 17:23:52 --- End: 12/12/2022, 17:23:52\n",
      "\t\tValidation: 0 -- Start: 12/12/2022, 17:23:52---End: 12/12/2022, 17:23:52\n",
      "\tFold: 1\n",
      "\tTRAIN: D:\\Github\\Stomach-Status-Classification\\Data set\\kmean_dataset\\10cv_128\\surf_train_50_4_3_True_Falsehisto_128_cv_index1.csv \n",
      "\tTEST: D:\\Github\\Stomach-Status-Classification\\Data set\\kmean_dataset\\10cv_128\\surf_test_50_4_3_True_Falsehisto_128_cv_index1.csv\n",
      "\t\tTraining : 1 -- Start: 12/12/2022, 17:23:52 --- End: 12/12/2022, 17:23:52\n",
      "\t\tValidation: 1 -- Start: 12/12/2022, 17:23:52---End: 12/12/2022, 17:23:52\n",
      "\tFold: 2\n",
      "\tTRAIN: D:\\Github\\Stomach-Status-Classification\\Data set\\kmean_dataset\\10cv_128\\surf_train_50_4_3_True_Falsehisto_128_cv_index2.csv \n",
      "\tTEST: D:\\Github\\Stomach-Status-Classification\\Data set\\kmean_dataset\\10cv_128\\surf_test_50_4_3_True_Falsehisto_128_cv_index2.csv\n",
      "\t\tTraining : 2 -- Start: 12/12/2022, 17:23:52 --- End: 12/12/2022, 17:23:52\n",
      "\t\tValidation: 2 -- Start: 12/12/2022, 17:23:52---End: 12/12/2022, 17:23:52\n",
      "\tFold: 3\n",
      "\tTRAIN: D:\\Github\\Stomach-Status-Classification\\Data set\\kmean_dataset\\10cv_128\\surf_train_50_4_3_True_Falsehisto_128_cv_index3.csv \n",
      "\tTEST: D:\\Github\\Stomach-Status-Classification\\Data set\\kmean_dataset\\10cv_128\\surf_test_50_4_3_True_Falsehisto_128_cv_index3.csv\n",
      "\t\tTraining : 3 -- Start: 12/12/2022, 17:23:52 --- End: 12/12/2022, 17:23:52\n",
      "\t\tValidation: 3 -- Start: 12/12/2022, 17:23:52---End: 12/12/2022, 17:23:52\n",
      "\tFold: 4\n",
      "\tTRAIN: D:\\Github\\Stomach-Status-Classification\\Data set\\kmean_dataset\\10cv_128\\surf_train_50_4_3_True_Falsehisto_128_cv_index4.csv \n",
      "\tTEST: D:\\Github\\Stomach-Status-Classification\\Data set\\kmean_dataset\\10cv_128\\surf_test_50_4_3_True_Falsehisto_128_cv_index4.csv\n",
      "\t\tTraining : 4 -- Start: 12/12/2022, 17:23:53 --- End: 12/12/2022, 17:23:53\n",
      "\t\tValidation: 4 -- Start: 12/12/2022, 17:23:53---End: 12/12/2022, 17:23:53\n",
      "\tFold: 5\n",
      "\tTRAIN: D:\\Github\\Stomach-Status-Classification\\Data set\\kmean_dataset\\10cv_128\\surf_train_50_4_3_True_Falsehisto_128_cv_index5.csv \n",
      "\tTEST: D:\\Github\\Stomach-Status-Classification\\Data set\\kmean_dataset\\10cv_128\\surf_test_50_4_3_True_Falsehisto_128_cv_index5.csv\n",
      "\t\tTraining : 5 -- Start: 12/12/2022, 17:23:53 --- End: 12/12/2022, 17:23:53\n",
      "\t\tValidation: 5 -- Start: 12/12/2022, 17:23:53---End: 12/12/2022, 17:23:53\n",
      "\tFold: 6\n",
      "\tTRAIN: D:\\Github\\Stomach-Status-Classification\\Data set\\kmean_dataset\\10cv_128\\surf_train_50_4_3_True_Falsehisto_128_cv_index6.csv \n",
      "\tTEST: D:\\Github\\Stomach-Status-Classification\\Data set\\kmean_dataset\\10cv_128\\surf_test_50_4_3_True_Falsehisto_128_cv_index6.csv\n",
      "\t\tTraining : 6 -- Start: 12/12/2022, 17:23:53 --- End: 12/12/2022, 17:23:53\n",
      "\t\tValidation: 6 -- Start: 12/12/2022, 17:23:53---End: 12/12/2022, 17:23:53\n",
      "\tFold: 7\n",
      "\tTRAIN: D:\\Github\\Stomach-Status-Classification\\Data set\\kmean_dataset\\10cv_128\\surf_train_50_4_3_True_Falsehisto_128_cv_index7.csv \n",
      "\tTEST: D:\\Github\\Stomach-Status-Classification\\Data set\\kmean_dataset\\10cv_128\\surf_test_50_4_3_True_Falsehisto_128_cv_index7.csv\n",
      "\t\tTraining : 7 -- Start: 12/12/2022, 17:23:53 --- End: 12/12/2022, 17:23:53\n",
      "\t\tValidation: 7 -- Start: 12/12/2022, 17:23:53---End: 12/12/2022, 17:23:53\n",
      "\tFold: 8\n",
      "\tTRAIN: D:\\Github\\Stomach-Status-Classification\\Data set\\kmean_dataset\\10cv_128\\surf_train_50_4_3_True_Falsehisto_128_cv_index8.csv \n",
      "\tTEST: D:\\Github\\Stomach-Status-Classification\\Data set\\kmean_dataset\\10cv_128\\surf_test_50_4_3_True_Falsehisto_128_cv_index8.csv\n",
      "\t\tTraining : 8 -- Start: 12/12/2022, 17:23:53 --- End: 12/12/2022, 17:23:53\n",
      "\t\tValidation: 8 -- Start: 12/12/2022, 17:23:53---End: 12/12/2022, 17:23:53\n",
      "\tFold: 9\n",
      "\tTRAIN: D:\\Github\\Stomach-Status-Classification\\Data set\\kmean_dataset\\10cv_128\\surf_train_50_4_3_True_Falsehisto_128_cv_index9.csv \n",
      "\tTEST: D:\\Github\\Stomach-Status-Classification\\Data set\\kmean_dataset\\10cv_128\\surf_test_50_4_3_True_Falsehisto_128_cv_index9.csv\n",
      "\t\tTraining : 9 -- Start: 12/12/2022, 17:23:53 --- End: 12/12/2022, 17:23:53\n",
      "\t\tValidation: 9 -- Start: 12/12/2022, 17:23:53---End: 12/12/2022, 17:23:53\n",
      "\n",
      "\n",
      "Traning Case: 4\n",
      "\tFold: 0\n",
      "\tTRAIN: D:\\Github\\Stomach-Status-Classification\\Data set\\kmean_dataset\\10cv_128\\surf_train_50_4_3_True_Falsehisto_128_cv_index0.csv \n",
      "\tTEST: D:\\Github\\Stomach-Status-Classification\\Data set\\kmean_dataset\\10cv_128\\surf_test_50_4_3_True_Falsehisto_128_cv_index0.csv\n",
      "\t\tTraining : 0 -- Start: 12/12/2022, 17:23:53 --- End: 12/12/2022, 17:23:53\n",
      "\t\tValidation: 0 -- Start: 12/12/2022, 17:23:53---End: 12/12/2022, 17:23:53\n",
      "\tFold: 1\n",
      "\tTRAIN: D:\\Github\\Stomach-Status-Classification\\Data set\\kmean_dataset\\10cv_128\\surf_train_50_4_3_True_Falsehisto_128_cv_index1.csv \n",
      "\tTEST: D:\\Github\\Stomach-Status-Classification\\Data set\\kmean_dataset\\10cv_128\\surf_test_50_4_3_True_Falsehisto_128_cv_index1.csv\n",
      "\t\tTraining : 1 -- Start: 12/12/2022, 17:23:53 --- End: 12/12/2022, 17:23:53\n",
      "\t\tValidation: 1 -- Start: 12/12/2022, 17:23:53---End: 12/12/2022, 17:23:53\n"
     ]
    }
   ],
   "source": [
    "training = Training(10, kmean_train_paths, kmean_test_paths)\n",
    "approach_dir = os.getcwd() + \"\\\\Experiment\\\\Approach\"\n",
    "svm_dir = approach_dir + \"\\\\LogisticRegression\"\n",
    "save_result_path = svm_dir + '\\\\lr_km64_surf50_result_fold{0}_0.9.pkl'\n",
    "training.train(path = save_result_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ab9bfc2",
   "metadata": {},
   "source": [
    "# Result Analyzing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfb40319",
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open(save_result_path.format(10), 'rb')\n",
    "result = pickle.load(file)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "214e9957",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of Training: {}\".format(len(list(result.keys()))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fbcfffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_case_lst = list(result.keys())\n",
    "print(training_case_lst[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6206d7b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sample = result[training_case_lst[0]]\n",
    "print(train_sample['param'])\n",
    "print(\"Number of Fold: {}\".format(len(train_sample['train_fold'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa1499eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "fold_sample = train_sample['train_fold']['fold_0']\n",
    "print(fold_sample['Classification Report'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3665699b",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_analyze_dict = {\n",
    "    'Train' : [],\n",
    "    'MACC' : [],\n",
    "    'MAP' : [],\n",
    "    'MASens' : [],\n",
    "    'MASpec' : [],\n",
    "    'MAF1' : [],\n",
    "    'AUC' : []\n",
    "}\n",
    "\n",
    "for result_key in result:\n",
    "    result_analyze_dict['Train'].append(result_key)\n",
    "    train_base = result[result_key]\n",
    "    fold_base = train_base['train_fold']\n",
    "    acc_lst, pre_lst, re_lst, spec_lst, f1_lst, auc_lst = [], [], [], [], [], []\n",
    "    for fold_key in fold_base:\n",
    "        current_fold = fold_base[fold_key]\n",
    "        current_fold_cls = current_fold['Classification Report']\n",
    "        acc_lst.append(current_fold_cls['accuracy'])\n",
    "        pre_lst.append(current_fold_cls['macro avg']['precision'])\n",
    "        re_lst.append(current_fold_cls['macro avg']['recall'])\n",
    "        spec_lst.append(current_fold['Confusion Matrix']['specificity'])\n",
    "        auc_lst.append(current_fold['ROC_AUC_SCORE']['Macro'])\n",
    "        f1_lst.append(current_fold_cls['macro avg']['f1-score'])\n",
    "    result_analyze_dict['MACC'].append(sum(acc_lst)/len(acc_lst))\n",
    "    result_analyze_dict['MAP'].append(sum(pre_lst)/len(pre_lst))\n",
    "    result_analyze_dict['MASens'].append(sum(re_lst)/len(re_lst))\n",
    "    result_analyze_dict['MASpec'].append(sum(spec_lst)/len(spec_lst))\n",
    "    result_analyze_dict['MAF1'].append(sum(f1_lst)/len(f1_lst))\n",
    "    result_analyze_dict['AUC'].append(sum(auc_lst)/len(auc_lst))\n",
    "\n",
    "result_df = pd.DataFrame(result_analyze_dict)\n",
    "result_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2656dd42",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_max = result_df.max()\n",
    "result_max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b82c192",
   "metadata": {},
   "outputs": [],
   "source": [
    "id_result_max = result_df.iloc[:, 1:].idxmax()\n",
    "id_result_max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85d60659",
   "metadata": {},
   "outputs": [],
   "source": [
    "id_max = 'train_{}'.format(id_result_max.mode()[0])\n",
    "print(id_max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb1dfb45",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df.loc[result_df['Train'] == id_max]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d8096bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_max = result[id_max]\n",
    "train_max['param']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0449dc73",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_max_fold = train_max['train_fold']\n",
    "train_max_analyze = {\n",
    "    'Fold' : [],\n",
    "    'ACC' : [],\n",
    "    'AP' : [],\n",
    "    'ASens' : [],\n",
    "    'ASpec' : [],\n",
    "    'AF1' : [],\n",
    "    'AUC' : []\n",
    "}\n",
    "\n",
    "for fold_key in train_max_fold:\n",
    "    train_max_analyze['Fold'].append(fold_key)\n",
    "    current_fold = train_max_fold[fold_key]\n",
    "    current_fold_cls = current_fold['Classification Report']\n",
    "    train_max_analyze['ACC'].append(current_fold_cls['accuracy'])\n",
    "    train_max_analyze['AP'].append(current_fold_cls['macro avg']['precision'])\n",
    "    train_max_analyze['ASens'].append(current_fold_cls['macro avg']['recall'])\n",
    "    train_max_analyze['ASpec'].append(current_fold['Confusion Matrix']['specificity'])\n",
    "    train_max_analyze['AF1'].append(current_fold_cls['macro avg']['f1-score'])\n",
    "    train_max_analyze['AUC'].append(current_fold['ROC_AUC_SCORE']['Macro'])\n",
    "\n",
    "train_max_analyze_df = pd.DataFrame(train_max_analyze)\n",
    "train_max_analyze_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9cc5ab6",
   "metadata": {},
   "source": [
    "# N_times_K_folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1525b39d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "\n",
    "train_max_param = train_max['param']\n",
    "n_times = 100\n",
    "\n",
    "nt_kf_output_dict = {\n",
    "    'Train' : [], # <index>_<fold>\n",
    "    'ACC' : [],\n",
    "    'AP' : [],\n",
    "    'ASens' : [],\n",
    "    'ASpec' : [],\n",
    "    'AF1' : [],\n",
    "    'AUC' : []\n",
    "}\n",
    "\n",
    "for n in range(n_times):\n",
    "    print(\"Training {}\".format(n))\n",
    "    fold_index = 0\n",
    "    for train_index, test_index in zip(kmean_train_paths, kmean_test_paths):\n",
    "        train_df = pd.read_csv(train_index)\n",
    "        test_df = pd.read_csv(test_index)      \n",
    "    \n",
    "        print(\"\\tFold: {}\".format(fold_index))\n",
    "        print(\"\\tTRAIN:\", train_index, \"\\n\\tTEST:\", test_index)\n",
    "\n",
    "        # folding data\n",
    "        x_train, y_train = train_df.iloc[:, 2:-1], train_df.iloc[:, -1]\n",
    "        x_test, y_test = test_df.iloc[:, 2:-1], test_df.iloc[:, -1]\n",
    "        \n",
    "        # Training\n",
    "        print(\"\\t\\tTraining : {}\".format(fold_index), end = \" -- \")\n",
    "        print(\"Start: {}\".format(datetime.now().strftime(\"%m/%d/%Y, %H:%M:%S\")), end=\" --- \")\n",
    "        model = LogisticRegression(penalty = train_max_param['penalty'],\n",
    "                                 fit_intercept = train_max_param['fit_intercept'],\n",
    "                                 class_weight = train_max_param['class_weight'],\n",
    "                                 solver = train_max_param['solver'])\n",
    "        model.fit(x_train,y_train)\n",
    "        print(\"End: {}\".format(datetime.now().strftime(\"%m/%d/%Y, %H:%M:%S\")))\n",
    "\n",
    "        # Testing\n",
    "        print(\"\\t\\tValidation: {}\".format(fold_index), end = \" -- \")\n",
    "        print(\"Start: {}\".format(datetime.now().strftime(\"%m/%d/%Y, %H:%M:%S\")), end=\"---\")\n",
    "        y_pred = model.predict(x_test)\n",
    "        print(\"End: {}\".format(datetime.now().strftime(\"%m/%d/%Y, %H:%M:%S\")))\n",
    "        \n",
    "        # Evaluation\n",
    "        cm = confusion_matrix(y_test, y_pred)\n",
    "        base_result = utils(cm, y_test, y_pred).get_value()\n",
    "        \n",
    "        # Result Storing\n",
    "        nt_kf_output_dict['Train'].append('{0}_{1}'.format(n, fold_index))\n",
    "        cls = base_result['Classification Report']\n",
    "        nt_kf_output_dict['ACC'].append(cls['accuracy'])\n",
    "        nt_kf_output_dict['AP'].append(cls['macro avg']['precision'])\n",
    "        nt_kf_output_dict['ASens'].append(cls['macro avg']['recall'])\n",
    "        nt_kf_output_dict['ASpec'].append(base_result['Confusion Matrix']['specificity'])\n",
    "        nt_kf_output_dict['AF1'].append(cls['macro avg']['f1-score'])\n",
    "        nt_kf_output_dict['AUC'].append(base_result['ROC_AUC_SCORE']['Macro'])\n",
    "        \n",
    "        # Fold index increment\n",
    "        fold_index += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fcfe405",
   "metadata": {},
   "outputs": [],
   "source": [
    "nt_kf_output_df = pd.DataFrame(nt_kf_output_dict)\n",
    "nt_kf_output_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b75b11c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "nt_kf_output_df.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd531675",
   "metadata": {},
   "outputs": [],
   "source": [
    "nt_kf_output_df.std(numeric_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d81114b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_values = nt_kf_output_df['ACC'].values.tolist()\n",
    "ap_values = nt_kf_output_df['AP'].values.tolist()\n",
    "asens_values = nt_kf_output_df['ASens'].values.tolist()\n",
    "aspec_values = nt_kf_output_df['ASpec'].values.tolist()\n",
    "af1_values = nt_kf_output_df['AF1'].values.tolist()\n",
    "auc_values = nt_kf_output_df['AUC'].values.tolist()\n",
    "print(len(acc_values), len(ap_values), len(asens_values), len(aspec_values), len(af1_values), len(auc_values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ac6a19d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats as st\n",
    "\n",
    "def CI(data, confidence_level, dis_type = 't'):\n",
    "    if dis_type == 't':\n",
    "        return st.t.interval(confidence=confidence_level, df=len(data)-1, \n",
    "                             loc=np.mean(data), scale=st.sem(data))\n",
    "    elif dis_type == 'g':\n",
    "        return st.norm.interval(confidence=confidence_level, loc=np.mean(data), scale=st.sem(data))\n",
    "\n",
    "def CI_calculator(confidence_level, dis_type = 't'):\n",
    "    for x in nt_kf_output_df.columns[1:]:\n",
    "        base = nt_kf_output_df[x].values.tolist()\n",
    "        print(\"CI of {}\".format(x), CI(base, confidence_level, dis_type))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b082a1d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "cl = 0.95\n",
    "\n",
    "CI_calculator(confidence_level = cl, dis_type = 't')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d8c9655",
   "metadata": {},
   "outputs": [],
   "source": [
    "cl = 0.95\n",
    "\n",
    "CI_calculator(confidence_level = cl, dis_type = 'g')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57fbb7cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "cl = 0.99\n",
    "\n",
    "CI_calculator(confidence_level = cl, dis_type = 'g')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
